---
title: "AE2"
output: html_document
date: "2025-01-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Pregunta 1:

Queremos programar un programa de tipo web scrapping con el que podamos obtener
una página web, mediante su URL, y poder analizar su contenido HTML con tal de extraer
datos e información específica.

Nuestro programa ha de ser capaz de cumplir con los siguientes pasos:
1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R
apto para ser tratado.
El primer paso para realizar tareas de crawling y scraping es poder descargar los
datos de la web. Para esto usaremos la capacidad de R y de sus librerías (httr y
XML) para descargar webs y almacenarlas en variables que podamos convertir
en un formato fácil de analizar (p.e. de HTML a XML).

2. Analizar el contenido de la web, buscando el título de la página (que en HTML
se etiqueta como “title”).
En las cabeceras web encontramos información como el título, los ficheros de
estilo visual, y meta-información como el nombre del autor de la página, una
descripción de esta, el tipo de codificación de esta, o palabras clave que indican
qué tipo de información contiene la página. Una vez descargada la página, y
convertida a un formato analizable (como XML), buscaremos los elementos de
tipo “title”. P.e. “<title>Titulo de Página</title>”.

3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se
etiquetan como “a”), buscando el texto del enlace, así como la URL.
Vamos a extraer, usando las funciones de búsqueda XML, todos los enlaces que
salen de esta página con tal de listarlos y poder descargarlas más tarde. Sabemos
que estos son elementos de tipo “<a>”, que tienen el atributo “href” para indicar
la URL del enlace. P.e. “<a href = ‘enlace’>Texto del Enlace</a>”. Del enlace
nos quedaremos con la URL de destino y con el valor del enlace (texto del
enlace).
Pistas:
§ Otra vez, usando xpathSApply() podemos buscar los enlaces, en los que
nos interesa el texto (valor del enlace) y la URL a la que apunta (atributo
“href”).

4. Generar una tabla con cada enlace encontrado, indicando el texto que
acompaña el enlace, y el número de veces que aparece un enlace con ese
mismo objetivo.
En este paso nos interesa reunir los datos obtenidos en el anterior paso.
Tendremos que comprobar, para cada enlace, cuantas veces aparece.

5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de
status HTTP al hacer una petición a esa URL).
En este paso podemos usar la función HEAD de la librería “httr”, que en vez de
descargarse la página como haría GET, solo consultamos los atributos de la
página o fichero destino.
HEAD nos retorna una lista de atributos, y de entre estos hay uno llamado
“header” que contiene más atributos sobre la página buscada. Si seguimos
podemos encontrar el “status_code” en “resultado$status_code”. El
“status_code” nos indica el resultado de la petición de página o fichero. Este
código puede indicar que la petición ha sido correcta (200), que no se ha
encontrado (404), que el acceso está restringido (403), etc.

• Tened en cuenta que hay enlaces con la URL relativa, con forma
“/xxxxxx/xxxxx/a.html”. En este caso, podemos indicarle como “handle” el
dominio de la página que estamos tratando, o añadirle el dominio a la URL
con la función “paste”.
• Tened en cuenta que puede haber enlaces externos con la URL absoluta, con
forma “http://xxxxxx/xxxx/a.html” (o https), que los trataremos
directamente.
• Tened en cuenta que puede haber enlaces que apunten a subdominios
distintos, con forma “//subdominio/xxxx/xxxx/a.html”. En este caso
podemos adjuntarle el prefijo “https:” delante, convirtiendo la URL en
absoluta.
• Tened en cuenta URLS internas con tags, como por ejemplo “#search-p”.
Estos apuntan a la misma página en la que estamos, pero diferente altura de
página. Equivale a acceder a la URL relativa de la misma página en la que
estamos.
Es recomendado poner un tiempo de espera entre petición y petición de pocos
segundos (comando “Sys.sleep”), para evitar ser “baneados” por el servidor. Para
poder examinar las URLs podemos usar expresiones regulares, funciones como
“grep”, o mirar si en los primeros caracteres de la URL encontramos “//” o “http”.
Para tratar las URLs podemos usar la ayuda de la función “paste”, para manipular
cadenas de caracteres y poder añadir prefijos a las URLs si fuera necesario.


Recomendaciones:
• Cuando tratéis datos y obtengáis resultados, guardadlos en variables en vez de
imprimirlos por pantalla solamente. De esta forma podéis reaprovechar los
resultados luego si fuera necesario.
• Si tenéis que iterar sobre vuestros datos, aprovechad las funciones de
vectorización de R. Por ejemplo, para sumar un vector no hagáis “for(i in 1:10) s
<- s + v[i]”, sino “s <- sum(v)”. Aprovechad también las diferentes funciones de la
familia apply(): (> ?apply).
• Si tenéis que filtrar filas o columnas de un data.frame, en vez de iterar con un
bucle, usad las funciones de subset: “[]” o $”.
• La página de ejemplo a analizar es:
https://www.mediawiki.org/wiki/MediaWiki
• Entenderemos que el dominio base es https://www.mediawiki.org.