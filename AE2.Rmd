---
title: "AE2"
output: html_document
date: "2025-01-13"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, attr.warning=F}
# Instalamos los paquetes necesarios
if (!require(httr)) install.packages("httr")
if (!require(XML)) install.packages("XML")

# Cargamos librerías
library(httr)
library(XML)
library(dplyr)
library(ggplot2)
library(gridExtra)
```

# Pregunta 1

### 1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.

```{r}
#Descargamos la página web
url <- "https://www.mediawiki.org/wiki/MediaWiki"
response <- GET(url)

# Revisamos el estado de la respuesta
if (status_code(response) == 200) {
  cat("Página descargada correctamente.\n")
} else {
  stop("Error al descargar la página. Código de estado: ", status_code(response))
}

# Lo convertimos de HTML a XML
html_content <- content(response, as = "text", encoding = "UTF-8")
xml_content <- htmlParse(html_content, encoding = "UTF-8")
```

Inicialmente descargamos la página web y comprobamos que lo hace bien.
Guardamos el contenido en HTML y en XML para posteriores manipulaciones.

### 2. Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”).

```{r}
# Extraemos el título de la página
page_title <- xpathSApply(xml_content, "//title", xmlValue)

# Mostramos el título
cat("El título de la página es", page_title, "\n")
```

Aprovechando las funcionalidades de xpathSApply extraemos el valor del
título de la página y lo mostramos.

### 3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL.

```{r}
# Extraemos los enlaces (<a>), sus atributos y el texto
link_texts <- xpathSApply(xml_content, "//a", xmlValue)

# Extraemos las URLs del atributo href
link_urls <- xpathSApply(xml_content, "//a/@href")

# Corregimos posibles valores NULL en los resultados
link_texts[is.null(link_texts)] <- NA
link_urls[is.null(link_urls)] <- NA

# Combinamos los textos y URLs en un data frame
links_df <- data.frame(
  Text = link_texts,
  URL = link_urls,
  stringsAsFactors = FALSE
)

# Mostrar una vista previa de los enlaces extraídos
print(head(links_df))
```

Primeramente extraemos el contenido de los enlaces y los enlaces para
crear un dataframe en el que se muestre el enlace y su contenido.

### 4. Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo.

```{r}
# Contamos las ocurrencias de cada combinación de texto y enlace
link_summary <- as.data.frame(table(links_df$Text, links_df$URL))
colnames(link_summary) <- c("Text", "URL", "Count")

# Filtramos sólo los enlaces con recuentos mayores a 0
link_summary <- link_summary[link_summary$Count > 0, ]

# Mostramos la tabla resultante
print(head(link_summary))
```

A partir del data frame anterior creamos una tabla con tres columna
(texto, enlace y contaje) que nos muestre el número de repeticiones de
cada uno de los enlaces (siempre que haya un mínimo de un enlace) para
mostrarlos posteriormente.

### 5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL).

```{r}
#Descargarmos de nuevo la web
base_url <- "https://www.mediawiki.org"

# Diferenciamos entre URLs relativas y absolutas
resolve_url <- function(link) {
  if (is.na(link)) return(NA)
  if (grepl("^http", link)) return(link)  # URL absoluta
  if (grepl("^//", link)) return(paste0("https:", link))  # Subdominio
  if (grepl("^/", link)) return(paste0(base_url, link))  # URL relativa
  if (grepl("^#", link)) return(base_url)  # Mismo documento
  return(NA)  # No válido
}

# Pasamos las URLs a forma vectorizada
resolved_urls <- sapply(link_urls, resolve_url)

# Creamos el data.frame con texto y URLs
links_df <- data.frame(
  Text = link_texts,
  URL = resolved_urls,
  stringsAsFactors = FALSE
)

# Contamos las repeticiones de cada enlace
link_summary <- links_df %>%
  count(Text, URL, name = "Seen")

# Función para obtener el estado HTTP de una URL
get_status <- function(link) {
  if (is.na(link)) return(NA)
  tryCatch({
    status_code(HEAD(link))
  }, error = function(e) NA)
}

# Obtenemos el estado HTTP de cada URL (vectorizado)
link_summary <- link_summary %>%
  mutate(Status = sapply(URL, get_status))

# Mostrar el resultado
print(link_summary)
```

Se descarga nuevamente la web para hacer una discriminación entre los
enlaces relativos y absolutos, y hacermos de nuevo el data frame y
contamos cuántas repeticiones tiene cada enlace, y su estado. Se lo
pasamos a la tabla link_summary y nos nostrará los valores de los
enlaces, su texto, repeticiones y estado.

# Pregunta 2

### 1. Un histograma con la frecuencia de aparición de los enlaces, pero separado porURLs absolutas (con “http…”) y URLs relativas.

```{r}
# Extraemos todos los enlaces
links <- xpathSApply(xml_content, "//a/@href")

# Clasificamos enlaces absolutos y relativos
absolute_links <- grep("^http", links, value = TRUE)
relative_links <- grep("^http", links, invert = TRUE, value = TRUE)

# Contamos frecuencias
frequencies <- data.frame(
  Tipo = c("Absoluta", "Relativa"),
  Count = c(length(absolute_links), length(relative_links))
)

# Generaramos el histograma
ggplot(frequencies, aes(x = Tipo, y = Count, fill = Tipo)) +
  geom_bar(stat = "identity") + ggtitle("Frecuencia de enlaces absolutos y relativos") +
  xlab("Tipo de enlace") + ylab("Frecuencia") + theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Primeramente extraemos los enlaces, los clasificamos en relativos y
absolutos, para con la función "frequencies" crear un data frame a
partir del cual generamos el histograma.

### 2. Un gráfico de barras indicando la suma de enlaces que apuntan a otros dominios o servicios (distinto a <https://www.mediawiki.org> en el caso de ejemplo) vs. la suma de los otros enlaces.

```{r}
# Clasificamos las URLs como internas o externas
classify_url <- function(url) {
  if (is.na(url)) {
    return("Desconocido")
  } else if (grepl(base_url, url)) {
    return("Internos")
  } else {
    return("Externos")
  }
}

# Las incluimos en el data frame
link_summary$Category <- sapply(link_summary$URL, classify_url)

# Sumamos enlaces internos y externos
url_sums <- aggregate(Seen ~ Category, data = link_summary, sum, na.rm = TRUE)

# Generamos el gráfico de barras
barplot(
  height = url_sums$Seen,names.arg = url_sums$Category,main = "Enlaces internos y externos",
  xlab = "Tipo de enlace",ylab = "Suma",col = c("salmon", "turquoise"), border=FALSE
)

```

Clasificamos las urls como internas o externas con la función grep y
posteriormente lo añadimos a la tabla linkSummary. Con la función
aggregate sumamos cuántos hay de cada tipo para mostrarlo en el gráfico
de barras.

### 3.Un gráfico de tarta (pie chart) indicando los porcentajes de Status de nuestro análisis.

```{r}
# Contamos las frecuencias de los estados
status_counts <- table(link_summary$Status)

# Calculamos porcentajes
status_percentages <- round(100 * status_counts / sum(status_counts, na.rm = TRUE), 1)

# Generamos las etiquetas con el código de estado y su porcentaje
status_labels <- paste(names(status_counts), "(", status_percentages, "%)", sep = "")

#Después de buscar los códigos de colores los asignamos
custom_colors <- c("200" = "#40E0D0", "404" = "#FFA07A")

# Generar el gráfico de tarta
pie1 <- pie(
  status_counts,labels = status_labels,col = custom_colors[names(status_counts)],
  border=FALSE, main = "Porcentajes de estados de los enlaces"
)

```

Contamos cuántos enlaces hay de cada estado (200 ó 404) y calculamos sus
porcentajes. Les asignamos etiquetas y generamos el gráfico de tarta.

### 4. Juntamos los gráficos en una única hoja.

```{r}
# Dividimos el espacio gráfico en 3 columnas
par(mfrow = c(1, 3))

# Llamamos a los tres gráficos
{
  # Gráfico 1
  barplot(height = frequencies$Count, names.arg = frequencies$Tipo, 
          main = "Frecuencia de enlaces", xlab = "Tipo", ylab = "Frecuencia",
          col = c("salmon", "turquoise"), border=FALSE)
  
  # Gráfico 2
  barplot(height = url_sums$Seen, names.arg = url_sums$Category, 
          main = "Enlaces internos y externos", xlab = "Tipo de enlace", ylab = "Suma",
          col = c("salmon", "turquoise"), border=FALSE)
  
  # Gráfico 3
  pie(status_counts, labels = status_labels, col = custom_colors[names(status_counts)],
  border=FALSE,main = "Porcentajes de estados")
}
```

Con la función "par" indicamos que hay una fila y tres columnas e
indicamos qué gráfico va en cada columna.
