---
title: "AE2"
output: html_document
date: "2025-01-13"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pregunta 1

Queremos programar un programa de tipo web scrapping con el que podamos
obtener una página web, mediante su URL, y poder analizar su contenido
HTML con tal de extraer datos e información específica.

Nuestro programa ha de ser capaz de cumplir con los siguientes pasos:

1.  Descargar la página web de la URL indicada, y almacenarlo en un
    formato de R apto para ser tratado. El primer paso para realizar
    tareas de crawling y scraping es poder descargar los datos de la
    web. Para esto usaremos la capacidad de R y de sus librerías (httr y
    XML) para descargar webs y almacenarlas en variables que podamos
    convertir en un formato fácil de analizar (p.e. de HTML a XML).

```{r}

```

2.  Analizar el contenido de la web, buscando el título de la página
    (que en HTML se etiqueta como “title”). En las cabeceras web
    encontramos información como el título, los ficheros de estilo
    visual, y meta-información como el nombre del autor de la página,
    una descripción de esta, el tipo de codificación de esta, o palabras
    clave que indican qué tipo de información contiene la página. Una
    vez descargada la página, y convertida a un formato analizable (como
    XML), buscaremos los elementos de tipo “title”. P.e. “<title>Titulo
    de Página</title>”.

```{r}

```

3.  Analizar el contenido de la web, buscando todos los enlaces (que en
    HTML se etiquetan como “a”), buscando el texto del enlace, así como
    la URL. Vamos a extraer, usando las funciones de búsqueda XML, todos
    los enlaces que salen de esta página con tal de listarlos y poder
    descargarlas más tarde. Sabemos que estos son elementos de tipo
    “<a>”, que tienen el atributo “href” para indicar la URL del enlace.
    P.e. “<a href = ‘enlace’>Texto del Enlace</a>”. Del enlace nos
    quedaremos con la URL de destino y con el valor del enlace (texto
    del enlace).

```{r}

```

4.  Generar una tabla con cada enlace encontrado, indicando el texto que
    acompaña el enlace, y el número de veces que aparece un enlace con
    ese mismo objetivo. En este paso nos interesa reunir los datos
    obtenidos en el anterior paso. Tendremos que comprobar, para cada
    enlace, cuantas veces aparece.

```{r}

```

5.  Para cada enlace, seguirlo e indicar si está activo (podemos usar el
    código de status HTTP al hacer una petición a esa URL). En este paso
    podemos usar la función HEAD de la librería “httr”, que en vez de
    descargarse la página como haría GET, solo consultamos los atributos
    de la página o fichero destino. HEAD nos retorna una lista de
    atributos, y de entre estos hay uno llamado “header” que contiene
    más atributos sobre la página buscada. Si seguimos podemos encontrar
    el “status_code” en “resultado\$status_code”. El “status_code” nos
    indica el resultado de la petición de página o fichero. Este código
    puede indicar que la petición ha sido correcta (200), que no se ha
    encontrado (404), que el acceso está restringido (403), etc.

```{r}

```

La página de ejemplo a analizar es:
<https://www.mediawiki.org/wiki/MediaWiki>

• Entenderemos que el dominio base es <https://www.mediawiki.org>.

# Pregunta 2

1.  Un histograma con la frecuencia de aparición de los enlaces, pero
    separado por URLs absolutas (con “http…”) y URLs relativas.

```{r}

```

2.  Un gráfico de barras indicando la suma de enlaces que apuntan a
    otros dominios o servicios (distinto a <https://www.mediawiki.org>
    en el caso de ejemplo) vs. la suma de los otros enlaces.

```{r}

```

3.  Un gráfico de tarta (pie chart) indicando los porcentajes de Status
    de nuestro análisis.

```{r}

```
