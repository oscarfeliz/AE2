---
title: "AE2"
output: html_document
date: "2025-01-13"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pregunta 1

### 1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.

```{r, attr.warning=F}
# Instalar los paquetes necesarios si no están instalados
if (!require(httr)) install.packages("httr")
if (!require(XML)) install.packages("XML")

# Cargar las librerías
library(httr)
library(XML)
library(dplyr)

# Descargar la página web
url <- "https://www.mediawiki.org/wiki/MediaWiki"
response <- GET(url)

# Comprobar el estado de la respuesta
if (status_code(response) == 200) {
  cat("Página descargada correctamente.\n")
} else {
  stop("Error al descargar la página. Código de estado: ", status_code(response))
}

# Convertir HTML a formato XML
html_content <- content(response, as = "text", encoding = "UTF-8")
xml_content <- htmlParse(html_content, encoding = "UTF-8")
```

### 2. Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”).

```{r}
# Extraer el título de la página usando XPath
page_title <- xpathSApply(xml_content, "//title", xmlValue)

# Mostrar el título
cat("El título de la página es:", page_title, "\n")
```

### 3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL.

```{r}
# Extraer los enlaces (<a>) y sus atributos
# Extraer el texto del enlace
link_texts <- xpathSApply(xml_content, "//a", xmlValue)

# Extraer los URLs del atributo href
link_urls <- xpathSApply(xml_content, "//a/@href")

# Manejar posibles valores NULL en los resultados
link_texts[is.null(link_texts)] <- NA
link_urls[is.null(link_urls)] <- NA

# Combinar los textos y URLs en un data frame
links_df <- data.frame(
  Text = link_texts,
  URL = link_urls,
  stringsAsFactors = FALSE
)

# Mostrar una vista previa de los enlaces extraídos
print(head(links_df))
```

### 4. Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo.

```{r}
# Contar ocurrencias de cada combinación de texto y enlace
link_summary <- as.data.frame(table(links_df$Text, links_df$URL))
colnames(link_summary) <- c("Text", "URL", "Count")

# Filtrar solo los enlaces con recuentos mayores a 0
link_summary <- link_summary[link_summary$Count > 0, ]

# Mostrar la tabla resultante
print(head(link_summary))
```

### 5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL).

```{r}
# Resolver URLs relativas y absolutas
base_url <- "https://www.mediawiki.org"
resolve_url <- function(link) {
  if (is.na(link)) return(NA)
  if (grepl("^http", link)) return(link)  # URL absoluta
  if (grepl("^//", link)) return(paste0("https:", link))  # Subdominio
  if (grepl("^/", link)) return(paste0(base_url, link))  # URL relativa
  if (grepl("^#", link)) return(base_url)  # Mismo documento
  return(NA)  # No válido
}

# Resolver URLs de forma vectorizada
resolved_urls <- sapply(link_urls, resolve_url)

# Crear un data.frame con texto y URLs
links_df <- data.frame(
  Text = link_texts,
  URL = resolved_urls,
  stringsAsFactors = FALSE
)

# Contar repeticiones de cada enlace
link_summary <- links_df %>%
  count(Text, URL, name = "Seen")

# Función para obtener el estado HTTP de una URL
get_status <- function(link) {
  if (is.na(link)) return(NA)
  tryCatch({
    status_code(HEAD(link))
  }, error = function(e) NA)
}

# Obtener el estado HTTP de cada URL (vectorizado)
link_summary <- link_summary %>%
  mutate(Status = sapply(URL, get_status))

# Mostrar el resultado
print(link_summary)

```

# Pregunta 2

### 1. Un histograma con la frecuencia de aparición de los enlaces, pero separado porURLs absolutas (con “http…”) y URLs relativas.

```{r}
#librerias
library(ggplot2)
library(gridExtra)
```

```{r}

#Extraer el tipo de página (absoluta, relativa)}
links_tipo_url <- xpathSApply(xml_content,"//a",xmlGetAttr,"href")

# Crear un data.frame con texto, URLs y tipo
links_df <- data.frame(
  Text = link_texts,
  URL = resolved_urls,
  Tipo = links_tipo_url,
  stringsAsFactors = FALSE
)

# Clasificar las URLs como absolutas o relativas
links_df$Tipo <- ifelse(grepl("^http", links_df$Tipo), "Absoluta", "Relativa")

barplot (height = table(links_df$Tipo), col = "deepskyblue3", main = "Frecuencua aparición enlaces")
```

### 

2.  Un gráfico de barras indicando la suma de enlaces que apuntan a
    otros dominios o servicios (distinto a <https://www.mediawiki.org>
    en el caso de ejemplo) vs. la suma de los otros enlaces.

```{r}
# Base URL
base_url <- "https://www.mediawiki.org"

# Función para clasificar las URLs como internas o externas
classify_url <- function(url) {
  if (is.na(url)) {
    return("Desconocido")
  } else if (grepl(base_url, url)) {
    return("Interna")
  } else {
    return("Externa")
  }
}

# Clasificar las URLs en el data frame
link_summary$Category <- sapply(link_summary$URL, classify_url)

# Sumar enlaces internos y externos
url_sums <- aggregate(Seen ~ Category, data = link_summary, sum, na.rm = TRUE)

# Crear gráfico de barras
barplot(
  height = url_sums$Seen,names.arg = url_sums$Category,main = "Enlaces Internos vs Externos",
  xlab = "Tipo de Enlace",ylab = "Suma de Enlaces",col = c("cadetblue2", "deepskyblue1"),
  border = "black"
)

```

### 3.Un gráfico de tarta (pie chart) indicando los porcentajes de Status de nuestro análisis.

```{r}
# Contar las frecuencias de cada código de estado
status_counts <- table(link_summary$Status)

# Calcular los porcentajes
status_percentages <- round(100 * status_counts / sum(status_counts, na.rm = TRUE), 1)

# Crear etiquetas con el código de estado y el porcentaje
status_labels <- paste(names(status_counts), "(", status_percentages, "%)", sep = "")

# Generar el gráfico de tarta
pie1 <- pie(status_counts,labels = status_labels,
    col = rainbow(length(status_counts)),main = "Porcentajes de Status de los Enlaces"
)

```

```{r}
# Combinar los gráficos en una sola figura
#grid.arrange(grobs = list(barplot1, barplot2, pie), ncol = 2)

# Dividir el dispositivo gráfico en 2 columnas y 2 filas
layout(matrix(c(1, 2, 3, 3), ncol = 2, byrow = TRUE))

# Primer barplot
barplot(height = table(links_df$Tipo), 
        col = "deepskyblue3", 
        main = "Frecuencia de Aparición de Enlaces")

# Segundo barplot
barplot(height = table(url_sums$Category), 
        col = "cadetblue2", 
        main = "Frecuencia de Enlaces por Visto")

# Pie chart
pie(table(link_summary$Status), 
    col = c("red", "blue2"), 
    main = "Distribución de Estados HTTP")
```
